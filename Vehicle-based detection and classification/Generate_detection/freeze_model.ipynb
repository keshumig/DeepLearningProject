{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6f0a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below libray imports the required Python modules for building a TensorFlow model using the \"argparse\" and\n",
    "#\"tensorflow\" packages, as well as the \"tensorflow.contrib.slim\" module which provides a collection\n",
    "#of useful functions and classes for building deep learning models using TensorFlow.\n",
    "\n",
    "#The \"argparse\" module is a standard Python library that provides a command-line argument parsing interface.\n",
    "\n",
    "#The \"tensorflow\" module is the core TensorFlow library that provides the building blocks for creating and 4\n",
    "#training deep learning models.\n",
    "\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32039af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##This code implements a residual network for image classification. The network is created using the TensorFlow Slim library, and consists of a number of residual blocks. Each block contains two convolutional layers with batch normalization and ReLU activation, followed by an addition operation that combines the input to the block\n",
    "##with the output of the second convolutional layer. \n",
    "\n",
    "\n",
    "##The _batch_norm_fn function is a helper function that applies batch normalization to its input tensor, and is used in the create_link and \n",
    "##create_inner_block functions to normalize the inputs to the convolutional layers.\n",
    "\n",
    "def _batch_norm_fn(x, scope=None):\n",
    "    if scope is None:\n",
    "        scope = tf.get_variable_scope().name + \"/bn\"\n",
    "    return slim.batch_norm(x, scope=scope)\n",
    "\n",
    "##The create_link function creates a residual block by applying batch normalization and ReLU activation to its\n",
    "#input, passing the result through a user-specified network_builder function, and then combining\n",
    "#the output of the network_builder function with the input to the block. \n",
    "\n",
    "def create_link(\n",
    "        incoming, network_builder, scope, nonlinearity=tf.nn.elu,\n",
    "        weights_initializer=tf.truncated_normal_initializer(stddev=1e-3),\n",
    "        regularizer=None, is_first=False, summarize_activations=True):\n",
    "    if is_first:\n",
    "        network = incoming\n",
    "    else:\n",
    "        network = _batch_norm_fn(incoming, scope=scope + \"/bn\")\n",
    "        network = nonlinearity(network)\n",
    "        if summarize_activations:\n",
    "            tf.summary.histogram(scope+\"/activations\", network)\n",
    "\n",
    "    pre_block_network = network\n",
    "    post_block_network = network_builder(pre_block_network, scope)\n",
    "\n",
    "    incoming_dim = pre_block_network.get_shape().as_list()[-1]\n",
    "    outgoing_dim = post_block_network.get_shape().as_list()[-1]\n",
    "    if incoming_dim != outgoing_dim:\n",
    "        assert outgoing_dim == 2 * incoming_dim, \\\n",
    "            \"%d != %d\" % (outgoing_dim, 2 * incoming)\n",
    "        projection = slim.conv2d(\n",
    "            incoming, outgoing_dim, 1, 2, padding=\"SAME\", activation_fn=None,\n",
    "            scope=scope+\"/projection\", weights_initializer=weights_initializer,\n",
    "            biases_initializer=None, weights_regularizer=regularizer)\n",
    "        network = projection + post_block_network\n",
    "    else:\n",
    "        network = incoming + post_block_network\n",
    "    return network\n",
    "\n",
    "##The create_inner_block function creates the convolutional layers for a residual block, applying batch normalization, \n",
    "##ReLU activation, and dropout to the first layer, and skipping the batch normalization and activation for the second layer.\n",
    "\n",
    "def create_inner_block(\n",
    "        incoming, scope, nonlinearity=tf.nn.elu,\n",
    "        weights_initializer=tf.truncated_normal_initializer(1e-3),\n",
    "        bias_initializer=tf.zeros_initializer(), regularizer=None,\n",
    "        increase_dim=False, summarize_activations=True):\n",
    "    n = incoming.get_shape().as_list()[-1]\n",
    "    stride = 1\n",
    "    if increase_dim:\n",
    "        n *= 2\n",
    "        stride = 2\n",
    "\n",
    "    incoming = slim.conv2d(\n",
    "        incoming, n, [3, 3], stride, activation_fn=nonlinearity, padding=\"SAME\",\n",
    "        normalizer_fn=_batch_norm_fn, weights_initializer=weights_initializer,\n",
    "        biases_initializer=bias_initializer, weights_regularizer=regularizer,\n",
    "        scope=scope + \"/1\")\n",
    "    if summarize_activations:\n",
    "        tf.summary.histogram(incoming.name + \"/activations\", incoming)\n",
    "\n",
    "    incoming = slim.dropout(incoming, keep_prob=0.6)\n",
    "\n",
    "    incoming = slim.conv2d(\n",
    "        incoming, n, [3, 3], 1, activation_fn=None, padding=\"SAME\",\n",
    "        normalizer_fn=None, weights_initializer=weights_initializer,\n",
    "        biases_initializer=bias_initializer, weights_regularizer=regularizer,\n",
    "        scope=scope + \"/2\")\n",
    "    return incoming\n",
    "\n",
    "##The residual_block function is a wrapper around the create_link and create_inner_block functions that sets default \n",
    "##values for the nonlinearity, weights initializer, bias initializer,\n",
    "##regularizer, increase_dim, and is_first arguments.\n",
    "\n",
    "def residual_block(incoming, scope, nonlinearity=tf.nn.elu,\n",
    "                   weights_initializer=tf.truncated_normal_initializer(1e3),\n",
    "                   bias_initializer=tf.zeros_initializer(), regularizer=None,\n",
    "                   increase_dim=False, is_first=False,\n",
    "                   summarize_activations=True):\n",
    "\n",
    "    def network_builder(x, s):\n",
    "        return create_inner_block(\n",
    "            x, s, nonlinearity, weights_initializer, bias_initializer,\n",
    "            regularizer, increase_dim, summarize_activations)\n",
    "\n",
    "    return create_link(\n",
    "        incoming, network_builder, scope, nonlinearity, weights_initializer,\n",
    "        regularizer, is_first, summarize_activations)\n",
    "##The _create_network function creates the entire residual network by applying two convolutional layers to the input image, \n",
    "##followed by three residual blocks.\n",
    "##The output of the fully connected layer is returned as the output of the network.\n",
    "\n",
    "def _create_network(incoming, reuse=None, weight_decay=1e-8):\n",
    "    nonlinearity = tf.nn.elu\n",
    "    conv_weight_init = tf.truncated_normal_initializer(stddev=1e-3)\n",
    "    conv_bias_init = tf.zeros_initializer()\n",
    "    conv_regularizer = slim.l2_regularizer(weight_decay)\n",
    "    fc_weight_init = tf.truncated_normal_initializer(stddev=1e-3)\n",
    "    fc_bias_init = tf.zeros_initializer()\n",
    "    fc_regularizer = slim.l2_regularizer(weight_decay)\n",
    "\n",
    "    def batch_norm_fn(x):\n",
    "        return slim.batch_norm(x, scope=tf.get_variable_scope().name + \"/bn\")\n",
    "\n",
    "    network = incoming\n",
    "    network = slim.conv2d(\n",
    "        network, 32, [3, 3], stride=1, activation_fn=nonlinearity,\n",
    "        padding=\"SAME\", normalizer_fn=batch_norm_fn, scope=\"conv1_1\",\n",
    "        weights_initializer=conv_weight_init, biases_initializer=conv_bias_init,\n",
    "        weights_regularizer=conv_regularizer)\n",
    "    network = slim.conv2d(\n",
    "        network, 32, [3, 3], stride=1, activation_fn=nonlinearity,\n",
    "        padding=\"SAME\", normalizer_fn=batch_norm_fn, scope=\"conv1_2\",\n",
    "        weights_initializer=conv_weight_init, biases_initializer=conv_bias_init,\n",
    "        weights_regularizer=conv_regularizer)\n",
    "\n",
    "   \n",
    "    network = slim.max_pool2d(network, [3, 3], [2, 2], scope=\"pool1\")\n",
    "\n",
    "    network = residual_block(\n",
    "        network, \"conv2_1\", nonlinearity, conv_weight_init, conv_bias_init,\n",
    "        conv_regularizer, increase_dim=False, is_first=True)\n",
    "    network = residual_block(\n",
    "        network, \"conv2_3\", nonlinearity, conv_weight_init, conv_bias_init,\n",
    "        conv_regularizer, increase_dim=False)\n",
    "\n",
    "    network = residual_block(\n",
    "        network, \"conv3_1\", nonlinearity, conv_weight_init, conv_bias_init,\n",
    "        conv_regularizer, increase_dim=True)\n",
    "    network = residual_block(\n",
    "        network, \"conv3_3\", nonlinearity, conv_weight_init, conv_bias_init,\n",
    "        conv_regularizer, increase_dim=False)\n",
    "\n",
    "    network = residual_block(\n",
    "        network, \"conv4_1\", nonlinearity, conv_weight_init, conv_bias_init,\n",
    "        conv_regularizer, increase_dim=True)\n",
    "    network = residual_block(\n",
    "        network, \"conv4_3\", nonlinearity, conv_weight_init, conv_bias_init,\n",
    "        conv_regularizer, increase_dim=False)\n",
    "\n",
    "    feature_dim = network.get_shape().as_list()[-1]\n",
    "    network = slim.flatten(network)\n",
    "\n",
    "    network = slim.dropout(network, keep_prob=0.6)\n",
    "    network = slim.fully_connected(\n",
    "        network, feature_dim, activation_fn=nonlinearity,\n",
    "        normalizer_fn=batch_norm_fn, weights_regularizer=fc_regularizer,\n",
    "        scope=\"fc1\", weights_initializer=fc_weight_init,\n",
    "        biases_initializer=fc_bias_init)\n",
    "\n",
    "    features = network\n",
    "\n",
    "    # Features in rows, normalize axis 1.\n",
    "    features = slim.batch_norm(features, scope=\"ball\", reuse=reuse)\n",
    "    feature_norm = tf.sqrt(\n",
    "        tf.constant(1e-8, tf.float32) +\n",
    "        tf.reduce_sum(tf.square(features), [1], keepdims=True))\n",
    "    features = features / feature_norm\n",
    "    return features, None\n",
    "\n",
    "\n",
    "def _network_factory(weight_decay=1e-8):\n",
    "\n",
    "    def factory_fn(image, reuse):\n",
    "            with slim.arg_scope([slim.batch_norm, slim.dropout],\n",
    "                                is_training=False):\n",
    "                with slim.arg_scope([slim.conv2d, slim.fully_connected,\n",
    "                                     slim.batch_norm, slim.layer_norm],\n",
    "                                    reuse=reuse):\n",
    "                    features, logits = _create_network(\n",
    "                        image, reuse=reuse, weight_decay=weight_decay)\n",
    "                    return features, logits\n",
    "\n",
    "    return factory_fn\n",
    "\n",
    "\n",
    "def _preprocess(image):\n",
    "    image = image[:, :, ::-1]  # BGR to RGB\n",
    "    return image\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"Parse command line arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Freeze old model\")\n",
    "    parser.add_argument(\n",
    "        \"--checkpoint_in\",\n",
    "        default=\"resources/networks/mars-small128.ckpt-68577\",\n",
    "        help=\"Path to checkpoint file\")\n",
    "    parser.add_argument(\n",
    "        \"--graphdef_out\",\n",
    "        default=\"resources/networks/mars-small128.pb\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    with tf.Session(graph=tf.Graph()) as session:\n",
    "        input_var = tf.placeholder(\n",
    "            tf.uint8, (None, 128, 64, 3), name=\"images\")\n",
    "        image_var = tf.map_fn(\n",
    "            lambda x: _preprocess(x), tf.cast(input_var, tf.float32),\n",
    "            back_prop=False)\n",
    "\n",
    "        factory_fn = _network_factory()\n",
    "        features, _ = factory_fn(image_var, reuse=None)\n",
    "        features = tf.identity(features, name=\"features\")\n",
    "\n",
    "        saver = tf.train.Saver(slim.get_variables_to_restore())\n",
    "        saver.restore(session, args.checkpoint_in)\n",
    "\n",
    "        output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            session, tf.get_default_graph().as_graph_def(),\n",
    "            [features.name.split(\":\")[0]])\n",
    "        with tf.gfile.GFile(args.graphdef_out, \"wb\") as file_handle:\n",
    "            file_handle.write(output_graph_def.SerializeToString())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
